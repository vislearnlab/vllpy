{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment these lines if you have not installed any packages yet\n",
    "# !pip install vislearnlabpy\n",
    "# !pip install git+https://github.com/openai/CLIP.git\n",
    "# !pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vislearnlabpy.embeddings.generate_embeddings import EmbeddingGenerator\n",
    "from vislearnlabpy.embeddings.embedding_store import EmbeddingStore\n",
    "from vislearnlabpy.embeddings.utils import display_search_results, zscore_embeddings \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = Path(os.getcwd()) / \"mongo_output\" # often set to /Volumes/vislearnlab/experiments/drawings/data..\n",
    "drawings_df = pd.read_csv(Path(SAVE_DIR / \"AllDescriptives_images_final_birch_run_v1.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming columns in the format our embedding generator expects them to be in\n",
    "drawings_df = drawings_df.rename(columns={\n",
    "    'filename': 'image1',\n",
    "    'category': 'text1'\n",
    "})\n",
    "# getting rid of articles\n",
    "drawings_df['text1'] = drawings_df['text1'].apply(lambda x: x.split('_')[-1])\n",
    "# Filtering to just our actual participants\n",
    "filtered_df = drawings_df[drawings_df['participantID'].str.lower().str.contains('bd')]\n",
    "filtered_df.to_csv(\"tmp_draw_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our embedding generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change device=\"cuda\" or \"cuda:2\" etc if you are using with GPU\n",
    "clip_generator = EmbeddingGenerator(model_type=\"clip\", device=\"cpu\", output_type=\"doc\") \n",
    "# setting text prompt to \"a drawing of a xx\" will make sure CLIP knows it's looking at drawings\n",
    "clip_generator.model.text_prompt = \"a drawing of a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_generator.generate_image_embeddings(output_path=\"sketch_embeddings\", input_csv=\"tmp_draw_df.csv\", batch_size=100, id_column=\"image1\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading our image embeddings and text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embedding_store = EmbeddingStore.from_doc(\"sketch_embeddings/image_embeddings/clip_image_embeddings_doc.docs\")\n",
    "text_embedding_store = EmbeddingStore.from_doc(\"sketch_embeddings/text_embeddings/clip_text_embeddings_doc.docs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just sanity checking the text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embedding_store.EmbeddingList.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search through our embedding store for 'sharks' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs, scores = image_embedding_store.search_store(text_query=\"shark\", limit=10)\n",
    "display_search_results(docs, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now only using drawings that were originally labeled as shark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs, scores = image_embedding_store.search_store(text_query=\"shark\", limit=10, categories=[\"shark\"])\n",
    "display_search_results(docs, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now see how recognizable different images are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vislearnlabpy.embeddings.similarity_utils import calculate_accuracy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "records = []\n",
    "for doc in image_embedding_store.EmbeddingList:\n",
    "    acc = calculate_accuracy(\n",
    "        doc.embedding,\n",
    "        text_embedding_store.EmbeddingList,\n",
    "        doc.text\n",
    "    )\n",
    "    records.append(\n",
    "        {\n",
    "            \"category\": doc.text,\n",
    "            \"accuracy\": acc,\n",
    "            \"url\": doc.url,          # can calculate age etc. here from ID extracted from URL\n",
    "        }\n",
    "    )\n",
    "df = pd.DataFrame(records)\n",
    "mean_df = (\n",
    "    df.groupby(\"category\", as_index=False)[\"accuracy\"]\n",
    "      .mean()\n",
    "      .query(\"accuracy > 0\")          # keep positives only\n",
    "      .sort_values(\"accuracy\")        # ascending for barh order\n",
    ")\n",
    "plt.figure(figsize=(8, max(4, len(mean_df) * 0.25)))\n",
    "plt.barh(mean_df[\"category\"], mean_df[\"accuracy\"]) # horizontal for readable labels\n",
    "plt.xlabel(\"Mean recognizability (probability)\")\n",
    "plt.title(\"Average recognizability per category\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save embeddings and recognizability values as CSV files for processing in R etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embedding_store.to_base_csv(\"sketch_embeddings/text_embeddings/clip_text_embeddings.csv\")\n",
    "image_embedding_store.to_base_csv(\"sketch_embeddings/image_embeddings/clip_image_embeddings.csv\")\n",
    "df.to_csv(\"recognizability.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z-score embeddings before saving if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embedding_store.EmbeddingList.normalized_embedding = zscore_embeddings(np.stack(image_embedding_store.EmbeddingList.embedding))\n",
    "# image_embedding_store.EmbeddingList.embedding = image_embedding_store.EmbeddingList.normalized_embedding\n",
    "# image_embedding_store.to_base_csv(\"sketch_embeddings/image_embeddings/clip_image_embeddings.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
