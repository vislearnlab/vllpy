{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment these lines if you have not installed any packages yet\n",
    "# !pip install vislearnlabpy\n",
    "# !pip install git+https://github.com/openai/CLIP.git\n",
    "# !pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To open in Google CoLab: https://colab.research.google.com/github/vislearnlab/vllpy/blob/main/examples/drawings/embedding_analysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vislearnlabpy.embeddings.generate_embeddings import EmbeddingGenerator\n",
    "from vislearnlabpy.embeddings.embedding_store import EmbeddingStore\n",
    "from vislearnlabpy.embeddings.utils import display_search_results, zscore_embeddings \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = Path(os.getcwd()) / \"mongo_output\" # often set to /Volumes/vislearnlab/experiments/drawings/data..\n",
    "drawings_df = pd.read_csv(Path(SAVE_DIR / \"AllDescriptives_images_final_birch_run_v1.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure the filename field is pointing to the right location, this is very important. Otherwise you will have to do some string manipulation to make sure it is\n",
    "drawings_df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap_path(full_path, new_base):\n",
    "    \"\"\"\n",
    "    Given a full path to a file and a new base directory, returns a new path\n",
    "    that keeps only the parent and grandparent directories of the file, prepended by new_base.\n",
    "\n",
    "    Example:\n",
    "        Input:\n",
    "            full_path = \".../mongo_output/sketches_full_dataset/square/image.png\"\n",
    "            new_base = \"/mnt/data/output\"\n",
    "\n",
    "        Output:\n",
    "            Path(\"/mnt/data/output/square/image.png\")\n",
    "    \"\"\"\n",
    "    original = Path(full_path)\n",
    "    parent = original.parent.name            # e.g. \"a_shark\"\n",
    "    \n",
    "    return str(Path(new_base) / parent / original.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming columns in the format our embedding generator expects them to be in\n",
    "drawings_df = drawings_df.rename(columns={\n",
    "    'filename': 'image1',\n",
    "    'category': 'text1'\n",
    "})\n",
    "# getting rid of articles\n",
    "drawings_df['text1'] = drawings_df['text1'].apply(lambda x: x.split('_')[-1])\n",
    "\n",
    "#remap path if needed: new_base=\"/file/storage/path\"\n",
    "#drawings_df[\"image1\"] = drawings_df[\"image1\"].apply(lambda x: remap_path(x, new_base))\n",
    "\n",
    "# Filtering to just our actual participants\n",
    "filtered_df = drawings_df[drawings_df['participantID'].str.lower().str.contains('bd')]\n",
    "filtered_df.to_csv(\"tmp_draw_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our embedding generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vislearnlabpy.embedding.stimuli_loader import ImgExtractionSettings, ImageExtractor\n",
    "clip_extraction_settings = ImgExtractionSettings(\n",
    "        resize_dim=224,\n",
    "        apply_content_crop=True,\n",
    "        apply_center_crop=False,\n",
    "        use_thumbnail=False,\n",
    "        background_threshold=1, # remove any small background artifacts that are size 1 or less\n",
    "        filter_edge_artifacts=False, \n",
    "        normalize_stroke_thickness=False\n",
    ")\n",
    "clip_transforms = ImageExtractor.get_transformations(clip_extraction_settings)\n",
    "# change device=\"cuda\" or \"cuda:2\" etc if you are using with GPU\n",
    "clip_generator = EmbeddingGenerator(model_type=\"clip\", device=\"cpu\", output_type=\"doc\", transform=clip_transforms) \n",
    "# setting text prompt to \"a drawing of a xx\" will make sure CLIP knows it's looking at drawings\n",
    "clip_generator.model.text_prompt = \"a drawing of a\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're storing our embeddings in DocArray lists so that they are accessible and loadable from a single file and are directly linked to their file paths and category names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_generator.generate_image_embeddings(output_path=\"sketch_embeddings\", input_csv=\"tmp_draw_df.csv\", batch_size=100, id_column=\"image1\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_generator.output_type = \"csv\"\n",
    "clip_generator.generate_image_embeddings(output_path=\"sketch_embeddings\", input_csv=\"tmp_draw_df.csv\", batch_size=100, id_column=\"image1\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading our image embeddings and text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embedding_store = EmbeddingStore.from_doc(\"sketch_embeddings/image_embeddings/clip_image_embeddings_doc.docs\")\n",
    "text_embedding_store = EmbeddingStore.from_doc(\"sketch_embeddings/text_embeddings/clip_text_embeddings_doc.docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embedding_store.EmbeddingList[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An RDM at the category level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embedding_store.compute_text_rdm(output_path=\"rdm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just sanity checking that our text embeddings look like we expect them to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embedding_store.EmbeddingList.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search through our embedding store for 'sharks' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs, scores = image_embedding_store.search_store(text_query=\"shark\", limit=10)\n",
    "display_search_results(docs, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now only using drawings that were originally labeled as shark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs, scores = image_embedding_store.search_store(text_query=\"penguin\", limit=10, categories=[\"penguin\"])\n",
    "display_search_results(docs, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = image_embedding_store.EmbeddingList[120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vislearnlabpy.embeddings.similarity_utils import calculate_accuracy\n",
    "calculate_accuracy(\n",
    "        a.embedding,\n",
    "        text_embedding_store.EmbeddingList,\n",
    "        a.text\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now see how recognizable different images are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vislearnlabpy.embeddings.similarity_utils import calculate_accuracy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "records = []\n",
    "for doc in image_embedding_store.EmbeddingList:\n",
    "    acc = calculate_accuracy(\n",
    "        doc.embedding,\n",
    "        text_embedding_store.EmbeddingList,\n",
    "        doc.text\n",
    "    )\n",
    "    records.append(\n",
    "        {\n",
    "            \"category\": doc.text,\n",
    "            \"accuracy\": acc,\n",
    "            \"url\": doc.url,          # can calculate age etc. here from ID extracted from URL\n",
    "        }\n",
    "    )\n",
    "df = pd.DataFrame(records)\n",
    "mean_df = (\n",
    "    df.groupby(\"category\", as_index=False)[\"accuracy\"]\n",
    "      .mean()\n",
    "      .query(\"accuracy > 0\")          # keep positives only\n",
    "      .sort_values(\"accuracy\")        # ascending for barh order\n",
    ")\n",
    "plt.figure(figsize=(8, max(4, len(mean_df) * 0.25)))\n",
    "plt.barh(mean_df[\"category\"], mean_df[\"accuracy\"]) # horizontal for readable labels\n",
    "plt.xlabel(\"Mean recognizability (probability)\")\n",
    "plt.title(\"Average recognizability per category\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save embeddings and recognizability values as CSV files for processing in R etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embedding_store.to_base_csv(\"sketch_embeddings/text_embeddings/clip_text_embeddings.csv\")\n",
    "image_embedding_store.to_base_csv(\"sketch_embeddings/image_embeddings/clip_image_embeddings.csv\")\n",
    "df.to_csv(\"recognizability.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z-score embeddings before saving if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embedding_store.EmbeddingList.normalized_embedding = zscore_embeddings(np.stack(image_embedding_store.EmbeddingList.embedding))\n",
    "# image_embedding_store.EmbeddingList.embedding = image_embedding_store.EmbeddingList.normalized_embedding\n",
    "# image_embedding_store.to_base_csv(\"sketch_embeddings/image_embeddings/clip_image_embeddings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vislearnlabpy.embeddings.similarity_generator import SimilarityGenerator\n",
    "sim_generator = SimilarityGenerator(similarity_type=\"cosine\", model=image_embedding_store.FeatureGenerator.model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_generator.cross_sims(image_embedding_store.EmbeddingList, text_embedding_store.EmbeddingList, output_csv=\"similarity_matrix.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
